# Multi-Agent Research & Synthesis System Specification

## System Purpose
Generate high-quality answers to complex, ambiguous problems by orchestrating differentiated AI agents through structured research and collaborative exploration phases.

**Target Problems**: Market predictions, unsolved scientific questions, complex philosophical inquiries, strategic decisions with high uncertainty.

---

## Architecture Overview

### Core Components
- **Flash LLM**: Fast processing for intent clarification, research planning, fact extraction
- **Pro LLM**: Advanced reasoning for orchestration, agent dialogue, synthesis
- **Search Infrastructure**: Google, arXiv, Reddit, Twitter APIs
- **Deep Sonar API**: Perplexity's deep research agent for targeted investigation
- **Agent Providers**: ChatGPT and Gemini for differentiated reasoning approaches

### Processing Flow
1. Intent Clarification → 2. Multi-Tier Research → 3. Agent Selection & Dialogue → 4. Iterative Refinement → 5. Final Synthesis

---

## Stage 1: Intent Clarification & Research Setup

### 1.1 Query Processing (Flash LLM)
**Input**: Raw user question
**Process**:
- Extract explicit requirements and constraints
- Identify answer format needed (prediction with confidence intervals, ranked options, decision framework, causal analysis)
- Generate clarifying questions for ambiguities
- Determine problem complexity level (affects subsequent resource allocation)

**Output**: Structured query specification or clarifying questions to user

### 1.2 Research Planning (Flash LLM)
**Input**: Clarified user intent
**Process**:
- Generate comprehensive search term sets for each research tier
- Define fact relevance scoring rubric specific to the question type
- Set evidence quality thresholds
- Identify domain-specific sources (relevant subreddits, expert Twitter accounts, specialized databases)

**Output**: 
- Search execution plan
- Fact scoring criteria
- Source priority rankings

---

## Stage 2: Three-Tier Research & Fact Compilation

### 2.1 Parallel Surface-Level Data Collection
**Surface-Level Sources**: Google, arXiv searches using generated terms
**Social Sentiment Sources**: Reddit (targeted subreddits), Twitter (expert accounts)

**Execution**: Surface-level and social searches run in parallel using generated terms

### 2.2 Initial Fact Extraction (Flash LLM)
**Input**: Raw search results from surface-level and social sources
**Process**:
- Extract unique claims with source attribution (prioritize recall over precision)
- Apply relevance scoring rubric to each claim
- Do NOT disregard any potentially relevant claim at this stage
- Generate comprehensive list of all claims with relevance scores

**Output**: Complete claim database with relevance scores and source lineage

### 2.3 Filtering & Analysis (Deterministic + Pro LLM)
**Step 1 - Deterministic Filter**:
- Apply relevance threshold to remove claims below minimum score
- Retain only claims scoring above threshold

**Step 2 - Pro LLM Analysis**:
**Input**: Filtered claims above threshold
**Process**:
- Generate deduplicated comprehensive list of claims
- Flag contradictory claims between sources
- Identify high-impact claims needing corroboration
- Highlight edge cases and boundary conditions requiring testing
- Map knowledge gaps with respect to user's question and follow-ups

**Output**: Structured analysis with claim categorization

### 2.4 Deep Research Query Generation (Flash LLM)
**Input**: Comprehensive claim analysis + user context
**Process**: Craft targeted question for Deep Sonar addressing:
- Corroboration needs for high-impact claims
- Resolution of contradictory findings
- Testing of identified edge cases
- Filling critical knowledge gaps

**Output**: Two artifacts:
- **Surface Research Report**: Comprehensive findings from surface-level and social research with sources
- **Deep Research Question**: Targeted query for Deep Sonar investigation

### 2.5 Deep Research Execution
**Process**: Execute Deep Sonar query using crafted question
**Output**: **Deep Research Report**: Targeted investigation results with sources

---

## Stage 3: Agent Selection & Dialogue Initialization

### 3.1 Strategic Agent Selection (Pro Orchestrator)
**Input**: Surface Research Report + Deep Research Report + user context

**Agent Provider Selection**: Choose one ChatGPT agent and one Gemini agent to leverage differentiated model architectures and training approaches

**Selection Criteria**: Choose agent configurations that maximize orthogonal search space exploration

**Agent Pairing Strategies**:
- **Cognitive Approach**: Inductive pattern-finder vs. Deductive framework-builder
- **Temporal Focus**: Short-term dynamics vs. Long-term structural analysis  
- **Evidence Weighting**: Empirical data maximizer vs. Theoretical model challenger
- **Risk Orientation**: Base-rate anchored vs. Tail-risk explorer

**Selection Process**:
1. Analyze problem structure to identify key uncertainty dimensions
2. Select agent configurations that create natural methodological tension
3. Assign one configuration to ChatGPT, one to Gemini
4. Generate specialized prompts that emphasize each agent's cognitive approach
5. Define success criteria for exploration completeness

**Output**: Two differentiated agent configurations (ChatGPT + Gemini) with clear success metrics

### 3.2 Agent Initialization
**Each Agent Receives**:
- Surface Research Report with source attribution
- Deep Research Report with source attribution
- User context and clarified intent
- Specialized cognitive framework prompt
- Exploration objectives (coverage, not consensus)

**Agent Behavior Rules**:
- Maintain truth-seeking objectivity above all else
- Generate hypotheses from assigned cognitive perspective
- Agree when evidence/logic is compelling
- Disagree with specific reasoning when warranted
- Build on counterpart insights while maintaining distinct viewpoint
- Attribute all claims to sources or label as speculation

---

## Stage 4: Iterative Dialogue & Refinement

### 4.1 Conversation Round Structure
**Agent Exchange Process**:
1. Each agent presents initial hypothesis with evidence
2. Agents challenge each other's reasoning and evidence interpretation
3. Agents build on valid insights while maintaining methodological differences
4. Agents identify areas requiring additional research or clarification

**Quality Controls**:
- Every claim must link to Surface/Deep Research Reports or be explicitly marked as speculation
- Disagreements must include specific reasoning, not arbitrary contrarianism
- New insights must be testable against available research evidence

### 4.2 Round Evaluation (Pro Judge)
**Input**: Complete research + user context + conversation history + success criteria

**Evaluation Decision Tree**:

**CONCLUDE** if:
- Success criteria satisfied (comprehensive exploration achieved)
- High-confidence answer emerged with strong evidence
- Maximum rounds reached (7)
- No new insights emerging for 2+ rounds

**CONTINUE** if:
- Significant knowledge gaps remain unexplored
- Agents discovering new important dimensions
- Evidence conflicts require deeper analysis
- Under maximum round limit with active insight generation

**Continue Actions**:
- Provide specific feedback to each agent
- Generate targeted questions to push exploration
- Request additional research if critical gaps identified
- No false consensus or pressure toward agreement

### 4.3 Alignment Check (Flash LLM)
**Trigger**: Before each continuation round
**Process**:
- Summarize conversation progress with fact lineage
- Assess risk of drift from user intent
- Generate human checkpoint question only if clear misalignment risk
- Otherwise proceed automatically

**Output**: Proceed signal or user clarification request

---

## Stage 5: Final Synthesis

### 5.1 Answer Construction (Pro LLM)
**Input**: Surface Research Report + Deep Research Report + conversation history + user context

**Synthesis Requirements**:
- Generate opinionated answer with clear reasoning chain
- Include confidence intervals or uncertainty bounds
- Preserve meaningful disagreements with supporting evidence
- Provide decision-relevant insights, not just information summary

**Output Structure**:
- **Executive Summary**: Direct answer to user question with confidence assessment
- **Evidence Foundation**: Key facts with source attribution supporting the conclusion
- **Reasoning Chain**: How research and agent dialogue led to final synthesis
- **Dissenting Views**: Where agents disagreed and why, with supporting evidence
- **Uncertainty Analysis**: What remains unknown and how it affects confidence
- **Source Audit**: Complete citation trail for all major claims

### 5.2 Quality Assurance
**Verification Checklist**:
- All major claims trace to sources in Surface/Deep Research Reports
- Disagreements are substantive, not artificial
- Answer directly addresses user's clarified intent
- Confidence levels reflect actual evidence strength
- Source attributions are accurate and accessible

---

## System Controls & Guardrails

### Operational Limits
- **Maximum Conversation Rounds**: 7
- **Agent Response Length**: 500-1000 words per turn
- **Research Time Budget**: Surface/Social parallel (5 min), Deep Sonar (10 min), Agent dialogue (15 min)

### Quality Assurance
- **Source Verification**: All citations must be accessible and accurately quoted
- **Speculation Labeling**: Any claim not directly supported by research must be explicitly marked
- **Disagreement Authentication**: Agent conflicts must stem from methodological differences, not random contrarianism
- **Answer Relevance**: Final synthesis must directly address user's clarified question

### Success Metrics
- **Search Space Coverage**: Percentage of identified uncertainty dimensions explored
- **Evidence Strength**: Quality and quantity of supporting sources
- **Intent Alignment**: User confirmation that answer addresses their actual question
- **Insight Novelty**: Discovery of non-obvious perspectives or solutions

---

## Implementation Notes

### Agent Differentiation Methods
- **Model Provider Diversity**: ChatGPT vs. Gemini for inherently different reasoning approaches
- **Cognitive Framework Prompting**: Explicit instructions to emphasize different analytical methodologies
- **Evidence Weighting Bias**: Training agents to weight quantitative vs. qualitative evidence differently

### Research Infrastructure
- **Sequential Processing**: Surface/Social parallel → Analysis → Deep Sonar → Agent dialogue
- **Source Quality Ranking**: Prioritize peer-reviewed, primary sources over aggregated content
- **Deep Sonar Integration**: Leverage Perplexity's specialized research capabilities for targeted investigation

### Scalability Considerations
- **Modular Architecture**: Each stage can be upgraded independently
- **Agent Provider Expansion**: Additional model providers can be integrated
- **Research Method Plugins**: Additional search tools can be integrated without system redesign